{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Step 2: Install and load rpy2\n",
        "!pip install rpy2\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects import pandas2ri\n",
        "import shutil\n",
        "\n",
        "# Enable the conversion from R to pandas DataFrame\n",
        "pandas2ri.activate()\n",
        "\n",
        "# Step 3: Copy the file from Google Drive to Colab environment\n",
        "source_path = '/content/drive/MyDrive/Data_Poli179/Data/tweet_level_dataset.rdata'\n",
        "destination_path = '/content/tweet_level_dataset.rdata'\n",
        "\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "# Step 4: Load the .RData file\n",
        "robjects.r['load'](destination_path)\n",
        "\n",
        "# Access the loaded R object\n",
        "tweet_level_dataset = robjects.globalenv['tweet_level_dataset']\n",
        "\n",
        "# Print the structure of the R object\n",
        "print(tweet_level_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPYdh9limOs4",
        "outputId": "d1cdc2e3-746c-41e7-d799-3a027c29a8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2) (1.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2) (3.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from rpy2) (2023.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2) (5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2) (2.1.5)\n",
            "                  id_anon EVENT_ID  predict_is_sympathy  predict_is_life  \\\n",
            "1         9xpjxThguYK4vj8    E8856             0.011034         0.009172   \n",
            "2         9xpjxThguYK4vj8    E8856             0.011034         0.009172   \n",
            "3         9xpjxThguYK4vj8    E0140             0.011034         0.009172   \n",
            "4         9xpjxThguYK4vj8    E0140             0.011034         0.009172   \n",
            "5         gHB1n7avBNqVcfq    E8856             0.024930         0.034532   \n",
            "...                   ...      ...                  ...              ...   \n",
            "15140863  Eeo1hho315hYqms    E0593             0.024930         0.034532   \n",
            "15140864  Eeo1hho315hYqms    E5027             0.024930         0.034532   \n",
            "15140865  Eeo1hho315hYqms    E5027             0.024930         0.034532   \n",
            "15140866  Eeo1hho315hYqms    E2845             0.024930         0.034532   \n",
            "15140867  Eeo1hho315hYqms    E2845             0.024930         0.034532   \n",
            "\n",
            "          predict_syria_travel_ff  predict_syrian_war  isis_topics  \\\n",
            "1                        0.007501            0.066806     0.094513   \n",
            "2                        0.007501            0.066806     0.094513   \n",
            "3                        0.007501            0.066806     0.094513   \n",
            "4                        0.007501            0.066806     0.094513   \n",
            "5                        0.017739            0.126672     0.203873   \n",
            "...                           ...                 ...          ...   \n",
            "15140863                 0.017739            0.211516     0.288718   \n",
            "15140864                 0.017739            0.211516     0.288718   \n",
            "15140865                 0.017739            0.211516     0.288718   \n",
            "15140866                 0.017739            0.211516     0.288718   \n",
            "15140867                 0.017739            0.211516     0.288718   \n",
            "\n",
            "          isis_topics_norm  post  in_event_area  \n",
            "1                 0.018787     0              0  \n",
            "2                 0.018787     0              0  \n",
            "3                 0.018787     0              0  \n",
            "4                 0.018787     0              0  \n",
            "5                 0.046263     0              0  \n",
            "...                    ...   ...            ...  \n",
            "15140863          0.067579     1              0  \n",
            "15140864          0.067579     1              0  \n",
            "15140865          0.067579     1              0  \n",
            "15140866          0.067579     1              0  \n",
            "15140867          0.067579     1              0  \n",
            "\n",
            "[15140867 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHAT GPT\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Step 2: Install and load rpy2\n",
        "!pip install rpy2\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects import pandas2ri\n",
        "import shutil\n",
        "\n",
        "# Enable the conversion from R to pandas DataFrame\n",
        "pandas2ri.activate()\n",
        "\n",
        "# Step 3: Copy the file from Google Drive to Colab environment\n",
        "source_path = '/content/drive/MyDrive/Data_Poli179/Data/en_tweets_datasets.rdata'\n",
        "destination_path = '/content/en_tweets_datasets.rdata'\n",
        "\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "# Step 4: Load the .RData file\n",
        "robjects.r['load'](destination_path)\n",
        "\n",
        "# List the objects in the R environment to see what was loaded\n",
        "r_objects = robjects.r.ls()\n",
        "print(r_objects)\n",
        "\n",
        "# Access the loaded R object\n",
        "en_tweets_datasets = robjects.globalenv['en_tweets_datasets']\n",
        "\n",
        "# Inspect the structure of the loaded object\n",
        "print(type(en_tweets_datasets))\n",
        "print(en_tweets_datasets.names)\n",
        "\n",
        "# Function to convert R objects to pandas DataFrames\n",
        "def convert_r_object_to_df(r_object):\n",
        "    if isinstance(r_object, robjects.vectors.DataFrame):\n",
        "        # If already a DataFrame, return it directly\n",
        "        return pandas2ri.rpy2py(r_object)\n",
        "    elif isinstance(r_object, robjects.vectors.ListVector):\n",
        "        # Handle the case where the object is a ListVector containing DataFrames\n",
        "        data_frames = {}\n",
        "        for key in r_object.names:\n",
        "            df = pandas2ri.rpy2py(r_object.rx2(key))\n",
        "            data_frames[key] = df\n",
        "        return data_frames\n",
        "    else:\n",
        "        print(\"Conversion not implemented for this type of R object.\")\n",
        "        return None\n",
        "\n",
        "# Convert the elements of the ListVector individually\n",
        "telegram_agg_df = en_tweets_datasets.rx('telegram_agg')[0]\n",
        "en_tweets_df = en_tweets_datasets.rx('en_tweets')[0]\n",
        "\n",
        "# Print the head of each DataFrame\n",
        "print(\"Head of telegram_agg DataFrame:\")\n",
        "print(telegram_agg_df.head())\n",
        "\n",
        "print(\"\\nHead of en_tweets DataFrame:\")\n",
        "print(en_tweets_df.head())"
      ],
      "metadata": {
        "id": "BFBhz_lUYp96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229bdd6e-3b0d-43b9-860e-265b617ef8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2) (1.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2) (3.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from rpy2) (2023.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2) (5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2) (2.1.5)\n",
            "[1] \"en_tweets_datasets\"\n",
            "\n",
            "<class 'rpy2.robjects.vectors.ListVector'>\n",
            "[1] \"telegram_agg\" \"en_tweets\"   \n",
            "\n",
            "Head of telegram_agg DataFrame:\n",
            "      date  sum\n",
            "1  16071.0  0.0\n",
            "2  16072.0  0.0\n",
            "3  16073.0  0.0\n",
            "4  16074.0  0.0\n",
            "5  16075.0  0.0\n",
            "\n",
            "Head of en_tweets DataFrame:\n",
            "           id_anon EVENT_ID  post  in_event_area  telegram  diff     date\n",
            "1  A0L8woX5UN1hEr6    E9955     0              0       0.0 -30.0  16868.0\n",
            "2  JCaNadHWNGBFuhG    E9955     0              0       0.0 -30.0  16868.0\n",
            "3  SPm8sWsBYOvh931    E9955     0              0       0.0 -30.0  16868.0\n",
            "4  MmIbKWOYs2cOHZ8    E9955     0              0       0.0 -30.0  16868.0\n",
            "5  jSLQjXZwDMAhKkv    E9955     0              0       0.0 -30.0  16868.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPlpM_STQ1bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "36e9a613-7f09-4673-eca7-3ba0232cbc69"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidSchema",
          "evalue": "No connection adapters were found for 'hhttps://drive.google.com/drive/folders/1Zdt305Kb3bT-1cgCmUdSAlqFmikMOyzb?usp=sharing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d7a0e56ef983>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fetch the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure we notice bad responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;31m# Get the appropriate adapter to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0madapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;31m# Start time (approximately) of the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget_adapter\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;31m# Nothing matches :-/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mInvalidSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No connection adapters were found for {url!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidSchema\u001b[0m: No connection adapters were found for 'hhttps://drive.google.com/drive/folders/1Zdt305Kb3bT-1cgCmUdSAlqFmikMOyzb?usp=sharing'"
          ]
        }
      ],
      "source": [
        "#import requests\n",
        "#import pandas as pd\n",
        "#from io import StringIO\n",
        "\n",
        "# URL to the data file\n",
        "#url = 'hhttps://drive.google.com/drive/folders/1Zdt305Kb3bT-1cgCmUdSAlqFmikMOyzb?usp=sharing'\n",
        "\n",
        "# Fetch the data\n",
        "#response = requests.get(url)\n",
        "#response.raise_for_status()  # Ensure we notice bad responses\n",
        "\n",
        "# Print the first 1000 characters to inspect the raw data\n",
        "#print(response.text[:1000])\n",
        "\n",
        "# Adjust the parsing based on the data inspection\n",
        "#try:\n",
        " #   data = pd.read_csv(StringIO(response.text), on_bad_lines='warn')\n",
        "#except pd.errors.ParserError as e:\n",
        " #   print(\"ParserError:\", e)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "#print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OMAR OMAR OMAR OMAR OMAR OMAR\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "P_3edZVCW6g-",
        "outputId": "ea89e93b-edbc-453e-846c-4fbc154c3a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/3rd Year/3rd Quarter/Poli 179/Data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d5368f09016e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/3rd Year/3rd Quarter/Poli 179/Data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/3rd Year/3rd Quarter/Poli 179/Data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
        "sns.set_context('notebook')\n",
        "sns.set_style(\"ticks\")\n",
        "from IPython.display import set_matplotlib_formats\n",
        "#set_matplotlib_formats('svg')\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "oIQ7lhjBP3z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Data Preprocessing\n",
        "First, install the necessary libraries:\n",
        "\n",
        "bash\n",
        "Copy code\n",
        "pip install pandas numpy nltk gensim\n",
        "Then, preprocess the data:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load data (replace with actual path)\n",
        "data = pd.read_csv(\"path/to/data.csv\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "# Define intervention date\n",
        "intervention_date = datetime.strptime(\"YYYY-MM-DD\", \"%Y-%m-%d\")\n",
        "\n",
        "# Split data\n",
        "pre_data = data[data['date'] < intervention_date]\n",
        "post_data = data[data['date'] >= intervention_date]"
      ],
      "metadata": {
        "id": "naVMdrYJP4Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Word Embeddings\n",
        "Use the gensim library to create word embeddings:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Ensure you have the NLTK stopwords and punkt tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "pre_data['tokens'] = pre_data['text'].apply(preprocess)\n",
        "post_data['tokens'] = post_data['text'].apply(preprocess)\n",
        "\n",
        "# Train Word2Vec models\n",
        "model_pre = Word2Vec(pre_data['tokens'], vector_size=100, window=5, min_count=2, workers=4)\n",
        "model_post = Word2Vec(post_data['tokens'], vector_size=100, window=5, min_count=2, workers=4)"
      ],
      "metadata": {
        "id": "6Y_g-XovQMp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Topic Clustering\n",
        "Use gensim for topic modeling with LDA:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from gensim import corpora\n",
        "\n",
        "# Create a dictionary and corpus for pre-intervention data\n",
        "dictionary_pre = corpora.Dictionary(pre_data['tokens'])\n",
        "corpus_pre = [dictionary_pre.doc2bow(tokens) for tokens in pre_data['tokens']]\n",
        "\n",
        "# Create a dictionary and corpus for post-intervention data\n",
        "dictionary_post = corpora.Dictionary(post_data['tokens'])\n",
        "corpus_post = [dictionary_post.doc2bow(tokens) for tokens in post_data['tokens']]\n",
        "\n",
        "# Fit LDA models\n",
        "lda_pre = models.LdaModel(corpus_pre, num_topics=5, id2word=dictionary_pre, passes=15)\n",
        "lda_post = models.LdaModel(corpus_post, num_topics=5, id2word=dictionary_post, passes=15)\n",
        "\n",
        "# Get topics\n",
        "topics_pre = lda_pre.print_topics(num_words=10)\n",
        "topics_post = lda_post.print_topics(num_words=10)"
      ],
      "metadata": {
        "id": "iX1CbCLJQWCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Comparison of Topics Over Time\n",
        "Compare the topics before and after the intervention:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Print results\n",
        "print(\"Pre-intervention topics:\")\n",
        "for topic in topics_pre:\n",
        "    print(topic)\n",
        "\n",
        "print(\"Post-intervention topics:\")\n",
        "for topic in topics_post:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "GhDCn2ZyQWRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}